{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch, torchvision, torch.nn as nn, torch.nn.functional as F\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Instance():\n",
    "    def __init__(self, text, label):\n",
    "        self.text = text.split(' ')\n",
    "        self.label = label\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def create(dataset, max_size=10000, min_freq=1):\n",
    "        text_vocab = Vocab()\n",
    "        label_vocab = Vocab()\n",
    "\n",
    "        text_vocab.stoi = {'<PAD>': 0, '<UNK>': 1}\n",
    "        text_vocab.itos = {0: '<PAD>', 1: '<UNK>'}\n",
    "\n",
    "        word_freq, label_freq = Vocab.calc_freqs(dataset)\n",
    "        Vocab.build_vocab(text_vocab, word_freq, max_size, min_freq)\n",
    "        Vocab.build_vocab(label_vocab, label_freq, max_size, min_freq, label=True)\n",
    "\n",
    "        return text_vocab, label_vocab \n",
    "\n",
    "    @staticmethod\n",
    "    def build_vocab(text_vocab, frequencies, max_size, min_freq, label=False):\n",
    "        frequencies = sorted(frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "        delta = 0 if label else 2\n",
    "\n",
    "        for i, (word, freq) in enumerate(frequencies):\n",
    "            if freq >= min_freq and (len(text_vocab.stoi) < max_size or max_size == -1):\n",
    "                text_vocab.stoi[word] = i + delta\n",
    "                text_vocab.itos[i + delta] = word\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    def encode(self, text):\n",
    "        if isinstance(text, str):\n",
    "            return torch.tensor(self.stoi.get(text, 1))\n",
    "        return torch.tensor([self.stoi.get(word, 1) for word in text])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            return self.itos[key]\n",
    "        elif isinstance(key, str):\n",
    "            return self.stoi[key]\n",
    "        else:\n",
    "            raise ValueError('key must be either int or str')\n",
    "\n",
    "    @staticmethod \n",
    "    def calc_freqs(dataset):\n",
    "        word_frequencies = {}\n",
    "        label_frequencies = {}\n",
    "        for instance in dataset:\n",
    "            for word in instance.text:\n",
    "                word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
    "            label_frequencies[instance.label] = label_frequencies.get(instance.label, 0) + 1\n",
    "        return word_frequencies, label_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, vocab=(None, None), max_size=-1, min_freq=1):\n",
    "        self.data = []\n",
    "        csv = pd.read_csv(path, sep=',', header=None)\n",
    "        for _, row in csv.iterrows():\n",
    "            self.data.append(Instance(row[0], row[1]))\n",
    "\n",
    "        if vocab[0] is not None:\n",
    "            self.text_vocab, self.label_vocab = vocab\n",
    "        else:\n",
    "            self.text_vocab, self.label_vocab = Vocab.create(self.data, max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_vocab.encode(self.data[idx].text), self.label_vocab.encode(self.data[idx].label)\n",
    "    \n",
    "    def instance(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocabular, path=None, embedding_dim=300):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocabular), embedding_dim, padding_idx=vocabular['<PAD>'])\n",
    "\n",
    "        emb = {}\n",
    "        if path is not None:\n",
    "            with open(path) as f:\n",
    "                for line in f:\n",
    "                    word, vec = line.split(' ', 1)\n",
    "                    emb[word] = torch.tensor([float(x) for x in vec.split()])\n",
    "        for i, word in vocabular.itos.items():\n",
    "            if word in emb:\n",
    "                self.embedding.weight.data[i] = emb[word]\n",
    "            elif word == '<PAD>':\n",
    "                self.embedding.weight.data[i] = torch.zeros(embedding_dim)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "class AveragePool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AveragePool, self).__init__()\n",
    "\n",
    "    def forward(self, x):    \n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.avg_pool1d(x, x.size(2))\n",
    "        x = x.squeeze(2)\n",
    "        return x\n",
    "    \n",
    "def pad_collate_fn(batch, pad_index=0):\n",
    "    texts, labels = zip(*batch)\n",
    "    lengths = [len(text) for text in texts]\n",
    "    max_len = max(lengths)\n",
    "    padded_texts = torch.stack([F.pad(text, (0, max_len - len(text)), value=pad_index) for text in texts])\n",
    "    return padded_texts, torch.tensor(labels), torch.tensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, args):\n",
    "    model.train()\n",
    "    with tqdm(data, unit='batch') as t:\n",
    "        for batch_num, batch in enumerate(t):\n",
    "            optimizer.zero_grad()\n",
    "            x, y, _ = batch\n",
    "            logits = model(x).squeeze(1)\n",
    "            loss = criterion(logits, y.float())\n",
    "            acc = ((logits > 0.5) == y).float().mean()\n",
    "            f1 = f1_score(logits > 0.5, y, zero_division=1)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip'])\n",
    "            optimizer.step()\n",
    "            t.set_postfix(loss=loss.item())\n",
    "            t.set_postfix(acc=acc.item())\n",
    "            t.set_postfix(f1=f1)\n",
    "    \n",
    "\n",
    "def evaluate(model, data, criterion, args):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc = 0\n",
    "        f1 = 0\n",
    "        avg_loss = 0\n",
    "        for batch_num, batch in enumerate(data):\n",
    "            x, y, _ = batch\n",
    "            logits = model(x).squeeze(1)\n",
    "            loss = criterion(logits, y.float())\n",
    "            acc += ((logits > 0.5) == y).float().mean()\n",
    "            f1 += f1_score(logits > 0.5, y, zero_division=1)\n",
    "            avg_loss += loss.item()\n",
    "        avg_loss /= len(data)\n",
    "        acc /= len(data)\n",
    "        f1 /= len(data)\n",
    "        print(f'Validation loss: {avg_loss}')\n",
    "        print(f'Validation accuracy: {acc}')\n",
    "        print(f'Validation F1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = {\n",
    "        'epochs': 5,\n",
    "        'clip': 1.0,\n",
    "        'seed': 7052020\n",
    "    }\n",
    "\n",
    "    train_dataset = NLPDataset('data/sst_train_raw.csv')\n",
    "    val_dataset = NLPDataset('data/sst_valid_raw.csv', vocab=(train_dataset.text_vocab, train_dataset.label_vocab))\n",
    "    test_dataset = NLPDataset('data/sst_test_raw.csv', vocab=(train_dataset.text_vocab, train_dataset.label_vocab))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=pad_collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=pad_collate_fn)\n",
    "\n",
    "    embedding = Embedding(train_dataset.text_vocab, path='data/sst_glove_6b_300d.txt', embedding_dim=300)\n",
    "\n",
    "    np.random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        embedding,\n",
    "        AveragePool(),\n",
    "        nn.Linear(300, 150),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(150, 150),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(150, 1)\n",
    "    )\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(args['epochs']):\n",
    "        train(model, train_loader, optimizer, loss_fn, args)\n",
    "        evaluate(model, val_loader, loss_fn, args)\n",
    "    evaluate(model, test_loader, loss_fn, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duboko",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
